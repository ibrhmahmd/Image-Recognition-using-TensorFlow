{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE7KNzPPVrVV"
      },
      "source": [
        "# Image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN7G9GFmVrVY"
      },
      "source": [
        "This tutorial shows how to classify images of flowers using a `tf.keras.Sequential` model and load data using `tf.keras.utils.image_dataset_from_directory`. It demonstrates the following concepts:\n",
        "\n",
        "\n",
        "* Efficiently loading a dataset off disk.\n",
        "* Identifying overfitting and applying techniques to mitigate it, including data augmentation and dropout.\n",
        "This tutorial follows a basic machine learning workflow:\n",
        "\n",
        "1. Examine and understand data\n",
        "2. Build an input pipeline\n",
        "3. Build the model\n",
        "4. Train the model\n",
        "5. Test the model\n",
        "6. Improve the model and repeat the process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF9uvbXNVrVY"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Import TensorFlow and other necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1WtoaOHVrVh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "import pathlib\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use this treminal commands to inastall all the libraries at once "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install numpy\n",
        "# pip install opencv\n",
        "# pip install PIL\n",
        "# pip install tensorflow\n",
        "# pip install matplotlib\n",
        "# pip install pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZZI6lNkVrVm"
      },
      "source": [
        "## Download and explore the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPHx8-t-VrVo"
      },
      "source": [
        "This tutorial uses a dataset of 3,670 photos of flowers. The dataset contains five sub-directories, one per class:\n",
        "\n",
        "```\n",
        "flower_photo/\n",
        "  daisy/\n",
        "  dandelion/\n",
        "  roses/\n",
        "  sunflowers/\n",
        "  tulips/\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57CcilYSG0zv"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\n",
        "data_dir = pathlib.Path(data_dir).with_suffix('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After downloading, you should now have a copy of the dataset available. There are 3,670 total images:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# loading the dataset Locally "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "local_dataset_path = \"PATH\\image recognition model\\\\flower_photos\"\n",
        "data_dir = pathlib.Path(local_dataset_path).with_suffix('').resolve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbtTDYhOHZb6"
      },
      "outputs": [],
      "source": [
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visaulizing a sample of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVmwkOSdHZ5A"
      },
      "source": [
        "Loading images of roses and plots the first two images using matplotlib, The images are opened using the PIL library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1loMlbYHeiJ"
      },
      "outputs": [],
      "source": [
        "roses = list(data_dir.glob('roses/*'))\n",
        "# Plot the first two images of roses\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(2):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    image = PIL.Image.open(str(roses[i]))\n",
        "    plt.imshow(image)\n",
        "    plt.title('Rose Image {}'.format(i + 1))\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGEqiBbRHnyI"
      },
      "source": [
        "loading images of tulips and plots the first two images using matplotlib, The images are opened using the PIL library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyQkfPGdHilw"
      },
      "outputs": [],
      "source": [
        "tulips = list(data_dir.glob('tulips/*'))\n",
        "# Plot the first two images of tulips\n",
        "plt.figure(figsize=(10, 5))    #initializes a new figure with a size of 10 inches by 5 inches.\n",
        "for i in range(2):    # iterates twice, for i values 0 and 1, to plot two images.\n",
        "    plt.subplot(1, 2, i + 1)    # sets up a subplot layout of 1 row and 2 columns, and selects the i-th subplot.\n",
        "    image = PIL.Image.open(str(tulips[i]))    # opens the i-th tulip image file.\n",
        "    plt.imshow(image)\n",
        "    plt.title('Tulip Image {}'.format(i + 1))    # This line sets the title for each subplot to 'Tulip Image 1' and 'Tulip Image 2'.\n",
        "    plt.axis('off')    # removes the axis labels and ticks from the plots.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIjgz7_JIo_m"
      },
      "source": [
        "## Load data using a Keras utility\n",
        "\n",
        "Next, load these images off disk using the helpful `tf.keras.utils.image_dataset_from_directory` utility. This will take you from a directory of images on disk to a `tf.data.Dataset` in just a couple lines of code. If you like, you can also write your own data loading code from scratch by visiting the [Load and preprocess images](../load_data/images.ipynb) tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyDNn9MbIzfT"
      },
      "source": [
        "### Create a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anqiK_AGI086"
      },
      "source": [
        "Define the initial parameters for the image processing:\n",
        "- batch_size : sets the number of images per batch. \n",
        "- img_height & img_width : define the dimensions to which each image will be resized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H74l2DoDI2XD"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "img_height = 180\n",
        "img_width = 180"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFBhRrrEI49z"
      },
      "source": [
        "It's good practice to use a validation split when developing your model. Use 80% of the images for training and 20% for validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `data_dir` : The directory where the data is located.\n",
        "- `validation_split=0.8` : Specifies that 80% of the data should be used for training.\n",
        "- `subset=\"training\"` : Indicates this is the training subset of the data.\n",
        "- `seed=123` : A random seed for shuffling and splitting the data, ensuring reproducibility.\n",
        "- `image_size=(img_height, img_width)` : Resizes all images to the specified height and width.\n",
        "- `batch_size=batch_size` : The number of images to include in each batch (64 images per batch).\n",
        "- `verbose=0` : Suppresses verbose output during dataset loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIR0kRZiI_AT"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.8,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `data_dir` : The directory where the data is located.\n",
        "- `validation_split=0.2` : Specifies that 20% of the data should be used for training.\n",
        "- `subset=\"validation\"` : Indicates this is the training subset of the data.\n",
        "- `seed=123` : A random seed for shuffling and splitting the data, ensuring reproducibility.\n",
        "- `image_size=(img_height, img_width)` : Resizes all images to the specified height and width.\n",
        "- `batch_size=batch_size` : The number of images to include in each batch (64 images per batch).\n",
        "- `verbose=0` : Suppresses verbose output during dataset loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iscU3UoVJBXj"
      },
      "outputs": [],
      "source": [
        "validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLQULyAvJC3X"
      },
      "source": [
        "You can find the class names in the `class_names` attribute on these datasets. These correspond to the directory names in alphabetical order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHAxkHX5JD3k"
      },
      "outputs": [],
      "source": [
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uoVvxSLJW9m"
      },
      "source": [
        "## Visualize the data\n",
        "\n",
        "Here are the first nine images from the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBmEA9c0JYes"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M6BXtXFJdW0"
      },
      "source": [
        "You will pass these datasets to the Keras `Model.fit` method for training later in this tutorial. If you like, you can also manually iterate over the dataset and retrieve batches of images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-MfMoenJi8s"
      },
      "outputs": [],
      "source": [
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj4FrKxxJkoW"
      },
      "source": [
        "The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 64 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.\n",
        "\n",
        "You can call `.numpy()` on the `image_batch` and `labels_batch` tensors to convert them to a `numpy.ndarray`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dr0at41KcAU"
      },
      "source": [
        "## Configure the dataset for performance\n",
        "\n",
        "Make sure to use buffered prefetching, so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data:\n",
        "\n",
        "- `Dataset.cache` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
        "- `Dataset.prefetch` overlaps data preprocessing and model execution while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOjJSm7DKoZA"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GUnmPF4JvEf"
      },
      "source": [
        "## Standardize the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e56VXHMWJxYT"
      },
      "source": [
        "The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network; in general you should seek to make your input values small.\n",
        "\n",
        "Here, you will standardize values to be in the `[0, 1]` range by using `tf.keras.layers.Rescaling`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEYxo2CTJvY9"
      },
      "outputs": [],
      "source": [
        "normalization_layer = layers.Rescaling(1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl4RmanbJ4g0"
      },
      "source": [
        "There are two ways to use this layer. You can apply it to the dataset by calling `Dataset.map`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9o9ESaJJ502"
      },
      "outputs": [],
      "source": [
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "first_image = image_batch[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pixel values are now in `[0,1]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"min value for pixel \", np.min(first_image))\n",
        "print(\"max value for pixel \", np.max(first_image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## processing the images\n",
        "To prepare it for input into the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `preprocess_image` function simplifies the preprocessing pipeline for image data, ensuring that images are properly formatted and ready for consumption by machine learning models.\n",
        "\n",
        "- `img_array = tf.keras.utils.img_to_array(img)`: converts the image to a NumPy array.\n",
        "- `img_array = tf.expand_dims(img_array, 0)` : extra dimension to the array to create a batch. This is necessary because many TensorFlow operations expect the batch dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = tf.keras.utils.load_img(image_path, target_size=(img_height, img_width))\n",
        "    img_array = tf.keras.utils.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    return img_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWEOmRSBJ9J8"
      },
      "source": [
        "Or, you can include the layer inside your model definition, which can simplify deployment. Use the second approach here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsRk1xCwKZR4"
      },
      "source": [
        "Note: You previously resized images using the `image_size` argument of `tf.keras.utils.image_dataset_from_directory`. If you want to include the resizing logic in your model as well, you can use the `tf.keras.layers.Resizing` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overfitting\n",
        "\n",
        "the training accuracy is increasing linearly over time, whereas validation accuracy stalls around 60% in the training process. Also, the difference in accuracy between training and validation accuracy is noticeable  a sign of [overfitting](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit).\n",
        "\n",
        "When there are a small number of training examples, the model sometimes learns from noises or unwanted details from training examples to an extent that it negatively impacts the performance of the model on new examples. This phenomenon is known as overfitting. It means that the model will have a difficult time generalizing on a new dataset.\n",
        "\n",
        "There are multiple ways to fight overfitting in the training process. In this tutorial, you'll use *data augmentation* and add *dropout* to your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data augmentation \n",
        "Overfitting generally occurs when there are a small number of training examples. `Data augmentation` takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `layers.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, 3))` : This layer applies random horizontal flips to the input images.\n",
        "`\"horizontal\"` specifies the direction of the flip.\n",
        "`input_shape=(img_height, img_width, 3)` defines the shape of the input images.( 3 color channels for RGB images).\n",
        "- `layers.RandomRotation(0.1)` : This layer applies random rotations to the input images.\n",
        "`0.1` specifies the maximum angle of rotation in radians.\n",
        "- `layers.RandomZoom(0.1)` : This layer applies random zooms to the input images.\n",
        "`0.1` specifies the range of zoom.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\",\n",
        "                          input_shape=(img_height,\n",
        "                                       img_width,\n",
        "                                       3)),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.1),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize a few augmented examples by applying data augmentation to the same image several times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize a few augmented examples by applying data augmentation to the same image several times\n",
        "plt.figure(figsize=(8, 8))\n",
        "for images, _ in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.title('Augmentation sample')\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a check point to callback later\n",
        "\n",
        "Preventing Loss of Progress: If training is interrupted, you can resume from the last saved checkpoint without starting from scratch.\n",
        "Model Performance: By saving the weights at each epoch or specified interval, you can keep track of the model's performance at different stages of training and potentially roll back to a better-performing set of weights if needed.\n",
        "\n",
        "`ModelCheckpoint` : This is a callback provided by Keras that saves the model or its weights at certain intervals during training.\n",
        "\n",
        "- `filepath=checkpoint_path` : Specifies the path where the checkpoint will be saved, as defined earlier.\n",
        "\n",
        "- `save_weights_only=True` : This parameter ensures that only the model's weights are saved, not the entire model architecture. This is useful if you want to save storage space and only need the weights to reload the model later.\n",
        "\n",
        "- `verbose=1` : This parameter enables verbose output, which means that information about the saving process will be printed to the console during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "checkpoint_path = \"PATH\\\\training_checkpoint/cp.weights.h5\"\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                    save_weights_only=True,\n",
        "                                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcUTyDOPKucd"
      },
      "source": [
        "# Training Up The model\n",
        "\n",
        "#### Create the model\n",
        "\n",
        "The Keras [Sequential](https://www.tensorflow.org/guide/keras/sequential_model) model consists of three convolution blocks (`tf.keras.layers.Conv2D`) with a max pooling layer (`tf.keras.layers.MaxPooling2D`) in each of them. There's a fully-connected layer (`tf.keras.layers.Dense`) with 128 units on top of it that is activated by a ReLU activation function (`'relu'`). This model has not been tuned for high accuracy; the goal of this tutorial is to show a standard approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR6argA1K074"
      },
      "outputs": [],
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "model = Sequential([\n",
        "    data_augmentation,\n",
        "        layers.Rescaling(1. / 255, input_shape=(img_height, img_width, 3)),\n",
        "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_classes, name=\"outputs\")\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`num_classes` : The total number of classes in the dataset, derived from the length of class_names.\n",
        "\n",
        "- `model = Sequential([...])`: linear stack of layers for building the neural network model.<br>\n",
        "list of layers is passed to the Sequential() constructor, specifying the layers of the model in sequential order.\n",
        "\n",
        "`Rescaling`\n",
        "- `(1. / 255)` : Scales the pixel values from the range [0, 255] to [0, 1].<br>\n",
        "- `input_shape=(img_height, img_width, 3)` : Specifies the shape of the input images (height, width, 3 color channels).\n",
        "\n",
        "\n",
        "1st convolutional layer<br>\n",
        "`Conv2D(16, 3)` : A convolutional layer with 16 filters, each of size 3x3.<br>\n",
        "`padding='same'` : Ensures the output has the same width and height as the input.<br>\n",
        "`activation='relu'` : Uses the ReLU activation function.\n",
        "\n",
        "1st Max Pooling layer<br>\n",
        "`MaxPooling2D()` : A max-pooling layer that reduces the spatial dimensions (height and width) by taking the maximum value in each 2x2 window.\n",
        "\n",
        "2nd convolutional layer<br>\n",
        "`Conv2D(32, 3)` : A convolutional layer with 32 filters, each of size 3x3.\n",
        "\n",
        "2nd Max Pooling layer<br>\n",
        "`MaxPooling2D()`\n",
        "\n",
        "3rd convolutional layer<br>\n",
        "`Conv2D(64, 3)` : A convolutional layer with 64 filters, each of size 3x3.\n",
        "\n",
        "3rd Max Pooling layer<br>\n",
        "`MaxPooling2D()`\n",
        "\n",
        "`Dropout(0.2)` : A dropout layer that randomly sets 20% of the input units to 0 at each update during training time to prevent overfitting.\n",
        "\n",
        "First Dense (Fully Connected) Layer:<br>\n",
        "`Dense(128)` : A fully connected layer with 128 neurons.\n",
        "\n",
        "Output Layer :\n",
        "- `Dense(num_classes)` : A fully connected layer with a neuron for each class in the dataset (output layer).\n",
        "- `name=\"outputs\"` : The name of the layer, typically used for identification purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaKFzz72Lqpg"
      },
      "source": [
        "## Compile the model\n",
        "\n",
        "For this tutorial, choose the `tf.keras.optimizers.Adam` optimizer and `tf.keras.losses.SparseCategoricalCrossentropy` loss function. To view training and validation accuracy for each training epoch, pass the `metrics` argument to `Model.compile`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`optimizer='adam'` :the Adam optimizer, which is an adaptive learning rate optimization algorithm. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.<br>\n",
        "\n",
        "\n",
        "`loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)x` :<br>\n",
        "Specifies the loss function to be used during training.\n",
        "- `SparseCategoricalCrossentropy` : This loss function is used for multi-class classification problems where the target variable is in integer form (sparse labels).\n",
        "- `from_logits=True` : Indicates that the model's output values are raw logits. This parameter ensures that the loss function applies the necessary transformation to the logits.\n",
        "\n",
        "\n",
        "`metrics=['accuracy']` : Specifies the metric to be evaluated by the model during training and testing.\n",
        "- `accuracy` : This metric calculates how often predictions match the labels. It is commonly used for classification problems to give a quick sense of model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMJ4DnuJL55A"
      },
      "source": [
        "## Model summary\n",
        "\n",
        "View all the layers of the network using the Keras `Model.summary` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llLYH-BXL7Xe"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiYHcbvaL9H-"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j30F69T4sIVN"
      },
      "source": [
        "Train the model for 50 epochs with the Keras `Model.fit` method:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fWToCqYMErH"
      },
      "outputs": [],
      "source": [
        "iterations = 50\n",
        "history = model.fit(\n",
        "      train_ds,\n",
        "      validation_data=validation_ds,\n",
        "      epochs=iterations,\n",
        "      callbacks=[checkpoint_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save&Load The Model:\n",
        "Saving the entire trained model to a file and attempts to load the weights from a specified checkpoint, handling potential errors if the checkpoint file is not found. It ensures the model's state can be preserved and reloaded for continued training or inference.<br>\n",
        "This allows you to reload the model later without needing to rebuild its architecture and ensures that the model uses the weights saved during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('my_model.keras')\n",
        "model.load_weights(checkpoint_path)\n",
        "try:\n",
        "    model.load_weights(checkpoint_path)\n",
        "    print(\"Weights loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Checkpoint file not found. Please verify the file path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`model.save('my_model.keras')` : Saves the entire model, including its architecture, weights, and training configuration, to a file named `my_model.keras`.<br>\n",
        "`model.load_weights(checkpoint_path)` : Loads the model's weights from the specified checkpoint file located at checkpoint_path.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyFKdQpXMJT4"
      },
      "source": [
        "## Visualize training results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFvOvmAmMK9w"
      },
      "source": [
        "Create plots of the loss and accuracy on the training and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWnopEChMMCn"
      },
      "outputs": [],
      "source": [
        "# Visualize training results accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "iterations_range = range(iterations)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(iterations_range, acc, label='Training Accuracy')\n",
        "plt.plot(iterations_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(iterations_range, loss, label='Training Loss')\n",
        "plt.plot(iterations_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtv5VbaVb-3W"
      },
      "source": [
        "## Predict on new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10buWpJbcCQz"
      },
      "source": [
        "Use your model to classify an image that wasn't included in the training or validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKgMZ4bDcHf7"
      },
      "source": [
        "Note: Data augmentation and dropout layers are inactive at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC40sRITBSsQ"
      },
      "outputs": [],
      "source": [
        "image_path = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
        "image_path = tf.keras.utils.get_file('Red_sunflower', origin=image_path)\n",
        "\n",
        "model = load_model(model_path)\n",
        "preprocessed_image = preprocess_image(image_path)\n",
        "predictions = model.predict(preprocessed_image)\n",
        "predicted_class_index = np.argmax(predictions)\n",
        "class_names = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
        "predicted_class = class_names[predicted_class_index]\n",
        "confidence = np.max(predictions) * 100\n",
        "\n",
        "print(f\"The image most likely belongs to {predicted_class} with a confidence of {confidence:.2f}%.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
